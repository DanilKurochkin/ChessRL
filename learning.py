from sb3_contrib import MaskablePPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import BaseCallback
from chess_env import SelfPlayChessEnv  # —Ç–≤–æ–π –∫–ª–∞—Å—Å —Å—Ä–µ–¥—ã


# Callback –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞
class OpponentUpdateCallback(BaseCallback):
    def __init__(self, env, update_freq=100_000, verbose=1):
        super().__init__(verbose)
        self.env = env
        self.update_freq = update_freq

    def _on_step(self) -> bool:
        if self.num_timesteps > 0 and self.num_timesteps % self.update_freq == 0:
            if self.verbose:
                print(f"\nüîÅ Updating opponent at timestep {self.num_timesteps}")
            self.model.save("latest_model")
            new_opponent = MaskablePPO.load("latest_model")
            self.env.set_attr("opponent_model", new_opponent)
        return True


def make_env():
    return Monitor(SelfPlayChessEnv(engine_depth=6))


vec_env = DummyVecEnv([make_env])

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —Å –≤–∫–ª—é—á–µ–Ω–Ω—ã–º tensorboard –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
model = MaskablePPO(
    "MlpPolicy",
    vec_env,
    verbose=1,
    tensorboard_log="./tensorboard_logs/"
)

# –ò–∑–Ω–∞—á–∞–ª—å–Ω–æ –∫–æ–ø–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –≤ –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞
model.save("latest_model")
initial_opponent = MaskablePPO.load("latest_model")
vec_env.set_attr("opponent_model", initial_opponent)

# Callback –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞
opponent_callback = OpponentUpdateCallback(vec_env, update_freq=100_000)

# –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è —Å callback
model.learn(total_timesteps=500_000, callback=opponent_callback)

model.save("selfplay_chess_final")
vec_env.close()
